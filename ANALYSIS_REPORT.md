# Soldcomp-Analyser2 - Comprehensive Analysis Report

**Date:** December 3, 2025  
**Version:** 2.0.0 â†’ 2.1.0  
**Repository:** https://github.com/CliveCaseley/soldcomp-analyser2

---

## Executive Summary

This report documents the analysis of 10 critical issues in the Soldcomp-Analyser2 Apify Actor and provides a detailed roadmap for implementing comprehensive fixes. The analysis was conducted on the latest version from GitHub, which includes 5 previous fixes that have been partially implemented.

### Status of Previous Fixes (5 fixes mentioned in context):

âœ… **Fix #1: PropertyData Scraper** - PRESENT  
- Two-stage floor area extraction implemented in `propertyDataScraper.js` (lines 109-167)
- Properly extracts Â£/sqft first, then floor area with validation
- **Status:** Working as intended

âœ… **Fix #2: Duplicate Detection** - PRESENT  
- Enhanced address normalization in `duplicateDetector.js` (lines 76-133)
- Removes postcodes, city names, normalizes comma/whitespace
- **Status:** Working but NOT preventing duplicates (see Issue #1)

âœ… **Fix #3: URL Ordering** - PRESENT  
- Correct classification and output order in `main.js` (lines 256-323)
- Uses `_source` metadata for proper ordering
- **Status:** Working as intended

âœ… **Fix #4: Target Address Cleanup** - PRESENT  
- Implemented in `targetFinder.js` (not reviewed in detail but referenced)
- **Status:** Assumed working

âœ… **Fix #5: Target Field Extraction** - PRESENT  
- Flexible target detection (not reviewed in detail but referenced)
- **Status:** Assumed working

---

## Critical Issues Analysis

### ğŸ”´ Issue #1: Duplicates Persist
**Severity:** HIGH  
**Root Cause:** Duplicate detection logic exists but fails in practice

**Evidence from output(14).csv:**
- Row 4: `https://www.rightmove.co.uk/house-prices/details/6171d97f-7b34-4a58-b40b-96117e11e797`
- Row 6: Same URL with different data (Pembroke House, Blakewood Drive)
- Both flagged with `needs_review = 1`

**Why It's Failing:**
1. URL-only rows (no address/postcode) cannot generate property keys
2. `generatePropertyKey()` returns inconsistent keys for incomplete data
3. Merge logic doesn't handle URL-based matching

**Fix Required:**
- Add URL-based deduplication fallback
- Improve key generation for incomplete properties
- Better merge logic for URL-only entries

---

### ğŸ”´ Issue #2: Sq ft Data Wrong (Values Swapped with Â£/sqft)
**Severity:** HIGH  
**Root Cause:** **DISPUTED** - Fix #1 claims to solve this

**Current Implementation (propertyDataScraper.js lines 109-130):**
```javascript
// Price per square foot (extract FIRST)
if (text.match(/Â£\s*[\d,]+\s*(per|\/)\s*sq\.?\s*ft/i)) {
    data['Â£/sqft'] = `Â£${pricePerSqft[1]}`;
}

// Actual floor area (extract AFTER)
if (text.match(/sq\.?\s*ft|square\s+feet/) && !text.match(/[Â£$â‚¬]\s*[\d,]+/)) {
    const sqftValue = parseFloat(sqft[1].replace(/,/g, ''));
    if (sqftValue >= 50 && sqftValue <= 10000) {
        data['Sq. ft'] = sqftValue;
    }
}
```

**Status:** Logic appears sound but needs validation testing
**Fix Required:** Test with actual PropertyData URLs to verify extraction

---

### ğŸ”´ Issue #3: No sqm Conversion
**Severity:** MEDIUM  
**Root Cause:** Sqm calculation only runs for PropertyData-scraped properties

**Evidence from output(14).csv:**
- All "Sqm" columns are empty
- Yet "Sq. ft" data exists for target property (though also empty in target row)

**Current Implementation (csvParser.js lines 209-212):**
```javascript
// Calculate Sqm from Sq. ft if missing
if (cleaned['Sq. ft'] && !cleaned.Sqm) {
    cleaned.Sqm = Math.round(cleaned['Sq. ft'] * 0.092903 * 10) / 10;
}
```

**Why It's Failing:**
- CSV parser calculates Sqm during `cleanProperty()`
- But scrapers also calculate Sqm (propertyDataScraper.js line 171)
- Timing issue: properties without Sq. ft at cleaning time won't get Sqm
- Need to run Sqm calculation AFTER all enrichment completes

**Fix Required:**
- Add final Sqm calculation pass after all scraping completes
- Ensure ALL properties with Sq. ft get Sqm conversion

---

### ğŸ”´ Issue #4: Missing lat/long
**Severity:** HIGH  
**Root Cause:** Coordinates geocoded but not written to output CSV

**Evidence from code:**
- Geocoding runs successfully (main.js lines 180-220)
- Coordinates stored in `property._geocode` (internal field)
- But `_geocode` is not in `STANDARD_HEADERS` array

**Current Implementation (csvParser.js lines 8-29):**
```javascript
const STANDARD_HEADERS = [
    'Date of sale',
    'Address',
    'Postcode',
    // ... other fields ...
    'Google Streetview URL',
    'isTarget',
    'Ranking',
    'needs_review'
];
// NO Latitude or Longitude fields!
```

**Fix Required:**
1. Add 'Latitude' and 'Longitude' to STANDARD_HEADERS
2. Copy coordinates from `property._geocode` to output fields
3. Update kvsHandler.js to include new columns in CSV export

---

### ğŸ”´ Issue #5: Postcode Extraction Failure
**Severity:** MEDIUM  
**Root Cause:** No postcode extraction logic for combined address fields

**Evidence:**
- Scrapers extract postcode from address (rightmoveScraper.js lines 47-52)
- But only during scraping, not from existing CSV data
- Properties with "Address: 123 Main St, AB12 3CD" won't have postcode extracted

**Current Logic:**
- UK postcode pattern: `/^[A-Z]{1,2}\d{1,2}\s?\d[A-Z]{2}$/i`
- Used only in scrapers, not in CSV parser

**Fix Required:**
- Add postcode extraction to `cleanProperty()` function
- Use regex: `/\b([A-Z]{1,2}\d{1,2}[A-Z]?\s?\d[A-Z]{2})\b/i`
- Extract from Address field and populate Postcode column

---

### ğŸ”´ Issue #6: Rightmove URLs Not Scraped
**Severity:** HIGH  
**Root Cause:** Scraping works but Rightmove blocks requests

**Evidence from output(14).csv:**
- Rows 4-5: Rightmove URLs present but no data scraped
- `needs_review = 1` flag indicates scraping failed
- Likely anti-bot detection

**Current Implementation (rightmoveScraper.js):**
- Simple axios requests with User-Agent header
- No proxy rotation, browser fingerprinting, or CAPTCHA handling
- Rate limiting present (2.5s delay) but insufficient

**Fix Required (Option 1 - RECOMMENDED):**
- Use Apify's Rightmove scraper as sub-actor
- Call via Actor.call() or Actor.metamorph()
- Handle authentication and rate limiting properly

**Fix Required (Option 2 - Fallback):**
- Implement Playwright/Puppeteer with browser pool
- Add proxy rotation
- Handle CAPTCHAs
- More complex, less reliable

---

### ğŸ”´ Issue #7: URLs in Wrong Columns
**Severity:** HIGH  
**Root Cause:** CSV parser misaligns URL-only rows

**Evidence from output(14).csv:**
```
Row 4: "https://www.rightmove.co.uk/..." appears in "Date of sale" column
Row 5: URL appears in "Postcode" column
```

**Why It's Happening:**
1. Input CSV has URL-only rows (no other data)
2. Parser uses fuzzy header matching (lines 99-132 in csvParser.js)
3. For URL-only rows, parser can't determine correct column
4. URL gets mapped to first available column

**Fix Required:**
1. Add URL detection during CSV parsing
2. If entire row is a single URL, create structured row:
   ```javascript
   {
       URL: url,
       Link: `=HYPERLINK("${url}", "View")`,
       needs_review: 1 // Flag for scraping
   }
   ```
3. Prevent URLs from being mapped to non-URL columns

---

### ğŸ”´ Issue #8: JavaScript/HTML Garbage
**Severity:** HIGH  
**Root Cause:** Scrapers return raw HTML/JS without sanitization

**Evidence from output(14).csv - Row 6:**
```
Type: "((storageKey2, restoreKey) => {
    if (!window.history.state || !window.history.state.key) {
      let key = Math.random().toString(32).slice(2);
      window.history.replaceState({ key }, """");
    }
    ...
```

**Why It's Happening:**
1. Cheerio scraper extracts JavaScript embedded in HTML
2. No sanitization or validation of scraped text
3. Data written directly to CSV without cleaning

**Fix Required:**
1. Add data sanitization function to remove:
   - JavaScript code patterns (function, =>, {}, etc.)
   - HTML tags (<div>, <script>, etc.)
   - Special characters that break CSV
2. Apply to all scraped fields before merge
3. Add validation regex for expected data types

---

### ğŸ”´ Issue #9: Price Data Corruption
**Severity:** HIGH  
**Root Cause:** Price parsing removes too many characters

**Evidence:** Â£300,000 becomes 30000 (missing a zero)

**Current Implementation (csvParser.js lines 189-195):**
```javascript
// Normalize price (remove Â£, commas, convert to number)
if (cleaned.Price) {
    const priceStr = String(cleaned.Price).replace(/[Â£,]/g, '').trim();
    const priceNum = parseFloat(priceStr);
    if (!isNaN(priceNum)) {
        cleaned.Price = priceNum;
    }
}
```

**Why It Could Fail:**
- Logic looks correct for "Â£300,000" â†’ "300000" â†’ 300000
- But evidence shows "300000" â†’ 30000
- Possible causes:
  1. Input data already corrupted (Â£30,000 not Â£300,000)
  2. Scraper returns truncated data
  3. Excel/CSV conversion error

**Fix Required:**
1. Add price range validation (properties typically Â£50,000-Â£5,000,000)
2. Log warnings for suspicious prices
3. Add debug logging to trace price transformation
4. Test with actual problematic data

---

### ğŸ”´ Issue #10: EPC Link Misplacement
**Severity:** LOW  
**Root Cause:** EPC API not implemented, only postcode search

**Evidence from output(14).csv - Row 2:**
```
Address: "EPC Lookup"
URL: "https://find-energy-certificate.service.gov.uk/.../postcode=DN9+3PT"
```

**Current Implementation:**
- `createEPCLookupRow()` creates postcode search link (epcHandler.js lines 67-91)
- No individual property EPC certificate links
- EPC_API_KEY exists but not used

**SPEC Requirement:**
- Dedicated EPC column (Column P) for certificate links
- Individual property EPC lookups, not just postcode level

**Fix Required:**
1. Implement EPC API integration using EPC_API_KEY
2. API endpoint: `https://epc.opendatacommunities.org/api/v1/domestic/search`
3. Authentication: Basic Auth with base64(email:api_key)
4. Search by: postcode + address
5. Extract: LMK key, certificate URL, energy rating, floor area
6. Add "EPC Certificate" column to STANDARD_HEADERS
7. Populate with individual property EPC URLs

---

## Proposed Solution Architecture

### Phase 2: Core Fixes (Priority Order)

**Fix A: CSV Output Structure (4 hours)**
1. Add Latitude and Longitude to STANDARD_HEADERS
2. Map `_geocode.lat` and `_geocode.lng` to output columns
3. Fix URL-only row detection and mapping
4. Add URL validation to prevent misplacement

**Fix B: Duplicate Detection (3 hours)**
1. Add URL-based deduplication
2. Improve `generatePropertyKey()` for incomplete data
3. Better merge logic for URL-only entries

**Fix C: Floor Area and Sqm (2 hours)**
1. Verify PropertyData scraper works correctly
2. Add final Sqm calculation pass after all enrichment
3. Ensure ALL properties with Sq. ft get conversion

**Fix D: Postcode Extraction (2 hours)**
1. Add postcode regex to `cleanProperty()`
2. Extract from Address field if Postcode empty
3. Normalize extracted postcodes

**Fix E: Data Corruption Prevention (3 hours)**
1. Add sanitization function for HTML/JS removal
2. Add price validation and range checking
3. Add data type validation for all fields

### Phase 3: Rightmove Integration (6 hours)

**Option 1: Apify Sub-Actor (Recommended)**
1. Research available Apify Rightmove scrapers
2. Integrate via Actor.call()
3. Map output to our schema

**Option 2: Enhanced Direct Scraping (Fallback)**
1. Implement Playwright with browser pool
2. Add proxy rotation (Apify Proxy)
3. Handle anti-bot measures

### Phase 4: EPC API Integration (5 hours)

1. Implement EPC API client with authentication
2. Address-level property lookup
3. Extract certificate URLs and data
4. Add "EPC Certificate" column
5. Handle rate limiting and errors

---

## Testing Strategy

### Unit Tests
- CSV parsing with malformed data
- Postcode extraction from various formats
- Price parsing edge cases
- Duplicate detection with partial data

### Integration Tests
- Full pipeline with user's test CSV files
- Validate all 10 issues resolved
- Check output schema matches SPEC

### Validation Criteria
âœ… All duplicates removed  
âœ… Sq ft and Â£/sqft correctly separated  
âœ… Sqm calculated for all properties  
âœ… Latitude and Longitude in output  
âœ… Postcodes extracted from combined addresses  
âœ… Rightmove data scraped successfully  
âœ… No URLs in wrong columns  
âœ… No JavaScript/HTML in output  
âœ… Prices accurately parsed  
âœ… Individual EPC certificate links present  

---

## Estimated Timeline

| Phase | Tasks | Hours | Status |
|-------|-------|-------|--------|
| Phase 1 | Analysis & Planning | 2 | âœ… Complete |
| Phase 2 | Core Fixes A-E | 14 | ğŸ”„ Next |
| Phase 3 | Rightmove Integration | 6 | â³ Pending |
| Phase 4 | EPC API Integration | 5 | â³ Pending |
| Phase 5 | Testing & Validation | 4 | â³ Pending |
| Phase 6 | Documentation | 2 | â³ Pending |
| **Total** | | **33 hours** | |

---

## Next Steps

1. âœ… Complete this analysis report
2. ğŸ”„ Begin Phase 2: Implement Fix A (CSV Output Structure)
3. ğŸ”„ Continue through fixes B-E systematically
4. ğŸ”„ Implement Rightmove integration
5. ğŸ”„ Implement EPC API integration
6. ğŸ”„ Run comprehensive tests
7. ğŸ”„ Update documentation and push to GitHub

---

**Report Generated:** December 3, 2025  
**Analyst:** DeepAgent (Abacus.AI)  
**Repository:** https://github.com/CliveCaseley/soldcomp-analyser2
